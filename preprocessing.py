import pandas as pd
import re
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import os

nltk.download('punkt')
nltk.download('stopwords')

# è½½å…¥åœç”¨è¯é›†åˆï¼Œåç»­å¯æ ¹æ®éœ€æ±‚è°ƒæ•´
stop_words = set(stopwords.words('english'))

# ä¿ç•™é‡‘èå¸¸ç”¨è¯ç¤ºä¾‹ï¼ˆä½ å¯ä»¥æ ¹æ®éœ€æ±‚æ‰©å……ï¼‰
finance_keep_words = {'no', 'not', 'buy', 'sell', 'hold', 'gain', 'loss'}

def clean_text(text):
    if not isinstance(text, str):
        return []

    text = text.lower()
    text = re.sub(r"http\S+", "", text)    # å»é™¤é“¾æ¥
    text = re.sub(r"@\w+", "", text)       # å»é™¤ @ç”¨æˆ·å
    text = re.sub(r"#", "", text)          # å»é™¤ #
    text = re.sub(r"[^a-z\s]", "", text)   # å»é™¤éå­—æ¯å­—ç¬¦ï¼ˆä¿ç•™ç©ºæ ¼ï¼‰

    tokens = word_tokenize(text)

    # è¿‡æ»¤åœç”¨è¯ï¼Œä½†ä¿ç•™é‡‘èå…³é”®åŠ¨è¯
    cleaned = [word for word in tokens if (word not in stop_words or word in finance_keep_words)]

    return cleaned

def preprocess_aligned_data(df_aligned):
    print(f"æ¸…æ´—å‰æ ·æœ¬æ•°: {len(df_aligned)}")

    # ä¿ç•™æ‰€æœ‰åˆ—ï¼Œè¿‡æ»¤ç¼ºå¤± TWEET å’Œ 1_DAY_RETURN çš„è¡Œ
    df = df_aligned.dropna(subset=['TWEET', '1_DAY_RETURN']).copy()

    print("æ­£åœ¨é¢„å¤„ç†æ–‡æœ¬...")

    df['cleaned_text'] = df['TWEET'].apply(clean_text)

    # è¿‡æ»¤æ‰æ¸…æ´—åç©ºæ–‡æœ¬çš„è¡Œ
    df = df[df['cleaned_text'].map(len) > 0].reset_index(drop=True)

    print(f"é¢„å¤„ç†å®Œæˆï¼Œæœ‰æ•ˆæ ·æœ¬æ•°: {len(df)}")
    print("æ¸…æ´—åå‰äº”è¡Œæ•°æ®ï¼š")

    # æ˜¾ç¤ºæ‰€æœ‰åˆ—ï¼Œæ–¹ä¾¿è°ƒè¯•
    pd.set_option('display.max_columns', None)
    print(df.head())
    pd.reset_option('display.max_columns')

    print("å½“å‰DataFrameåˆ—åï¼š")
    print(df.columns.tolist())

    # æ‰“å°å½“å‰å·¥ä½œç›®å½•ï¼Œæ–¹ä¾¿ç¡®è®¤æ–‡ä»¶ä¿å­˜ä½ç½®
    print("å½“å‰å·¥ä½œç›®å½•æ˜¯:", os.getcwd())

    # ä¿å­˜æ¸…æ´—åçš„æ•°æ®
    df.to_csv('cleaned_aligned_data.csv', index=False)
    print("å·²ä¿å­˜æ¸…æ´—åçš„æ•°æ®åˆ° cleaned_aligned_data.csv")

    return df

def load_and_preprocess(filepath="data/cleaned_aligned_data.csv"):
    import os

    if os.path.exists(filepath):
        print(f"ğŸ“‚ ç›´æ¥åŠ è½½å·²å­˜åœ¨çš„æ¸…æ´—æ•°æ®ï¼š{filepath}")
        return pd.read_csv(filepath, parse_dates=['DATE'])  # æ³¨æ„åˆ—å DATE æ˜¯å¤§å†™
    else:
        print(f"âš™ï¸ æ‰¾ä¸åˆ°æ¸…æ´—æ•°æ®ï¼Œå°è¯•é‡æ–°æ¸…æ´—ï¼š{filepath}")
        df_aligned = pd.read_csv(filepath, parse_dates=['DATE'])  # åŸå§‹æ•°æ®è·¯å¾„
        return preprocess_aligned_data(df_aligned)



# import pandas as pd
# import re
# import nltk
# from nltk.corpus import stopwords
# from nltk.tokenize import word_tokenize

# nltk.download('punkt')
# nltk.download('stopwords')

# stop_words = set(stopwords.words('english'))

# def clean_text(text):
#     if not isinstance(text, str):
#         return []
#     text = text.lower()
#     text = re.sub(r"http\S+", "", text)  # å»é™¤é“¾æ¥
#     text = re.sub(r"@\w+", "", text)     # å»é™¤ @ç”¨æˆ·å
#     text = re.sub(r"#", "", text)        # å»é™¤ #
#     text = re.sub(r"[^a-z\s]", "", text) # å»é™¤éå­—æ¯å­—ç¬¦ï¼ˆä¿ç•™ç©ºæ ¼ï¼‰
#     tokens = word_tokenize(text)
#     # ä¸è¿‡æ»¤æ‰€æœ‰ stopwordsï¼Œä¿ç•™ 'no', 'not'
#     cleaned = [word for word in tokens if word not in stop_words or word in ['no', 'not']]
#     return cleaned

# def preprocess_aligned_data(df_aligned):
#     print(f"æ¸…æ´—å‰æ ·æœ¬æ•°: {len(df_aligned)}")

#     # ä¿ç•™æ‰€æœ‰åˆ—ï¼Œè¿‡æ»¤æ‰ TWEET å’Œ 1_DAY_RETURN ç¼ºå¤±çš„è¡Œ
#     df = df_aligned.dropna(subset=['TWEET', '1_DAY_RETURN']).copy()

#     print("æ­£åœ¨é¢„å¤„ç†æ–‡æœ¬...")

#     df['cleaned_text'] = df['TWEET'].apply(clean_text)

#     # è¿‡æ»¤æ‰æ¸…æ´—åç©ºæ–‡æœ¬çš„è¡Œ
#     df = df[df['cleaned_text'].map(len) > 0].reset_index(drop=True)

#     print(f"é¢„å¤„ç†å®Œæˆï¼Œæœ‰æ•ˆæ ·æœ¬æ•°: {len(df)}")
#     print("æ¸…æ´—åå‰äº”è¡Œæ•°æ®ï¼š")

#     # è®¾ç½®æ˜¾ç¤ºæ‰€æœ‰åˆ—ï¼Œæ‰“å°å‰5è¡Œ
#     pd.set_option('display.max_columns', None)
#     print(df.head())
#     pd.reset_option('display.max_columns')  # æ¢å¤é»˜è®¤

#     print("å½“å‰DataFrameåˆ—åï¼š")
#     print(df.columns.tolist())

#     return df



# import pandas as pd
# import re
# import nltk
# from nltk.corpus import stopwords
# from nltk.tokenize import word_tokenize

# nltk.download('punkt')
# nltk.download('stopwords')

# stop_words = set(stopwords.words('english'))

# def clean_text(text):
#     if not isinstance(text, str):
#         return []
#     text = text.lower()
#     text = re.sub(r"http\S+", "", text)  # å»é™¤é“¾æ¥
#     text = re.sub(r"@\w+", "", text)     # å»é™¤ @ç”¨æˆ·å
#     text = re.sub(r"#", "", text)        # å»é™¤ #
#     text = re.sub(r"[^a-z\s]", "", text) # å»é™¤éå­—æ¯å­—ç¬¦ï¼ˆä¿ç•™ç©ºæ ¼ï¼‰
#     tokens = word_tokenize(text)
#     # ä¸è¦è¿‡æ»¤æ‰€æœ‰ stopwordsï¼Œä¿ç•™ä¸€éƒ¨åˆ†å¸¸è§è¯ï¼ˆä¾‹å¦‚ not, no å¯èƒ½å¯¹é‡‘èæƒ…æ„Ÿå¾ˆå…³é”®ï¼‰
#     cleaned = [word for word in tokens if word not in stop_words or word in ['no', 'not']]
#     return cleaned

# def load_and_preprocess(file_path):
#     df = pd.read_csv(file_path)
#     df = df[['TWEET', '1_DAY_RETURN']].dropna()
#     print("åŸå§‹æ ·æœ¬æ•°:", len(df))

#     print("æ­£åœ¨é¢„å¤„ç†æ–‡æœ¬...")
#     df['cleaned_text'] = df['TWEET'].apply(clean_text)

#     # è¿‡æ»¤æ‰ç©ºæ–‡æœ¬ï¼ˆé¢„å¤„ç†åé•¿åº¦ä¸º0ï¼‰
#     df = df[df['cleaned_text'].map(len) > 0]

#     print("é¢„å¤„ç†å®Œæˆï¼Œæœ‰æ•ˆæ ·æœ¬æ•°:", len(df))
#     return df
