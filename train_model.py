import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import Lasso
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import mean_squared_error, r2_score
from preprocessing import load_and_preprocess

def join_words(word_list):
    """å°†è¯åˆ—è¡¨è½¬ä¸ºä¸€å¥å­—ç¬¦ä¸²ä¾›Tfidfå¤„ç†"""
    return ' '.join(word_list)

def build_features(df, max_features=5000):
    """æ„å»ºTF-IDFå‘é‡"""
    df['text_joined'] = df['cleaned_text'].apply(join_words)
    df = df[df['text_joined'].str.strip() != '']
    df = df[df['1_DAY_RETURN'].notna()]
    print("å‰©ä½™æœ‰æ•ˆæ ·æœ¬æ•°:", len(df))
    print("\nç¤ºä¾‹æ–‡æœ¬ï¼š\n", df['text_joined'].iloc[0])
    
    vectorizer = TfidfVectorizer(max_features=max_features)
    X = vectorizer.fit_transform(df['text_joined'])
    y = df['1_DAY_RETURN'].values
    return X, y, vectorizer

def tune_lasso(X, y):
    """ä½¿ç”¨GridSearchCVå¯»æ‰¾æœ€ä¼˜çš„alphaå‚æ•°"""
    print("å¼€å§‹è°ƒå‚...")
    alphas = np.logspace(-5, -1, 10)
    model = Lasso(max_iter=10000)
    grid = GridSearchCV(model, {'alpha': alphas}, cv=5, scoring='r2')
    grid.fit(X, y)
    print(f"æœ€ä½³ alpha: {grid.best_params_['alpha']:.1e}")
    print(f"æœ€ä½³äº¤å‰éªŒè¯ RÂ²: {grid.best_score_:.4f}")
    return grid.best_estimator_

def report(model, X_test, y_test, vectorizer):
    """è¯„ä¼°æ¨¡å‹è¡¨ç°å¹¶è¾“å‡ºé‡è¦ç‰¹å¾"""
    y_pred = model.predict(X_test)
    mse = mean_squared_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)

    print(f"\nğŸ“Š æ¨¡å‹æµ‹è¯•é›†å‡æ–¹è¯¯å·®(MSE): {mse:.6f}")
    print(f"ğŸ“ˆ æ¨¡å‹æµ‹è¯•é›†R2å¾—åˆ†: {r2:.4f}")

    # è¾“å‡ºé‡è¦ç‰¹å¾
    coef = model.coef_
    features = vectorizer.get_feature_names_out()
    important_features = [(f, c) for f, c in zip(features, coef) if abs(c) > 1e-5]
    important_features = sorted(important_features, key=lambda x: abs(x[1]), reverse=True)

    if important_features:
        print("\nğŸ§  æœ€é‡è¦çš„è¯å’Œå¯¹åº”ç³»æ•°:")
        for word, weight in important_features[:20]:
            print(f"{word}: {weight:.6f}")
    else:
        print("\nâš ï¸ æ²¡æœ‰éé›¶é‡è¦ç‰¹å¾ï¼ˆæ¨¡å‹å¯èƒ½æ²¡å­¦åˆ°ä»€ä¹ˆï¼‰")

def plot_sentiment_vs_return(model, X_test, y_test):
    """æŒ‰é¢„æµ‹å¾—åˆ†åˆ†ç»„ï¼Œç»˜åˆ¶å¹³å‡å›æŠ¥è¶‹åŠ¿å›¾"""
    predicted_scores = model.predict(X_test)
    df = pd.DataFrame({'pred_score': predicted_scores, 'actual_return': y_test})
    # è§£å†³é‡å¤è¾¹ç•Œçš„é—®é¢˜ï¼Œduplicates='drop'
    df['decile'] = pd.qcut(df['pred_score'], 10, labels=False, duplicates='drop')

    avg_returns = df.groupby('decile')['actual_return'].mean()

    plt.figure(figsize=(10, 6))
    plt.plot(avg_returns.index, avg_returns.values, marker='o', label='Avg. Next-Day Return')
    plt.title("ğŸ“Š Predicted Sentiment Score vs. Avg. Next-Day Return", fontsize=16)
    plt.xlabel("Sentiment Score Group (Negative â†’ Positive)", fontsize=14)
    plt.ylabel("Average Actual Next-Day Return", fontsize=14)
    plt.grid(True)
    plt.xticks(range(len(avg_returns)), [f"Q{i+1}" for i in range(len(avg_returns))], fontsize=12)
    plt.yticks(fontsize=12)
    plt.legend(fontsize=12)
    plt.tight_layout()
    plt.show()

    print("\nå„åˆ†ç»„å¹³å‡æ¬¡æ—¥å›æŠ¥ï¼š")
    for i, r in enumerate(avg_returns):
        print(f"Q{i+1}: {r:.6f}")

def main():
    print("å¼€å§‹åŠ è½½å¹¶é¢„å¤„ç†æ•°æ®...")
    df = load_and_preprocess("data/reduced_dataset-release.csv")

    print("è½¬æ¢æ–‡æœ¬æ ¼å¼ï¼Œå‡†å¤‡ç‰¹å¾æå–...")
    X, y, vectorizer = build_features(df)

    print("æ‹†åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†...")
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    print("è®­ç»ƒå¹¶è°ƒå‚ Lasso æ¨¡å‹...")
    model = tune_lasso(X_train, y_train)

    print("è¯„ä¼°æ¨¡å‹...")
    report(model, X_test, y_test, vectorizer)

    print("åˆ†ææƒ…æ„Ÿå¾—åˆ†ä¸å›æŠ¥çš„å…³ç³»...")
    plot_sentiment_vs_return(model, X_test, y_test)

if __name__ == "__main__":
    main()


# import pandas as pd
# import numpy as np
# import matplotlib.pyplot as plt
# from sklearn.feature_extraction.text import TfidfVectorizer
# from sklearn.linear_model import Lasso
# from sklearn.model_selection import train_test_split, GridSearchCV
# from sklearn.metrics import mean_squared_error, r2_score
# from preprocessing import load_and_preprocess

# def join_words(word_list):
#     """å°†è¯åˆ—è¡¨è½¬ä¸ºä¸€å¥å­—ç¬¦ä¸²ä¾›Tfidfå¤„ç†"""
#     return ' '.join(word_list)

# def build_features(df, max_features=5000):
#     """æ„å»ºTF-IDFå‘é‡"""
#     df['text_joined'] = df['cleaned_text'].apply(join_words)
#     df = df[df['text_joined'].str.strip() != '']
#     df = df[df['1_DAY_RETURN'].notna()]
#     print("å‰©ä½™æœ‰æ•ˆæ ·æœ¬æ•°:", len(df))
#     print("\nç¤ºä¾‹æ–‡æœ¬ï¼š\n", df['text_joined'].iloc[0])
    
#     vectorizer = TfidfVectorizer(max_features=max_features)
#     X = vectorizer.fit_transform(df['text_joined'])
#     y = df['1_DAY_RETURN'].values
#     return X, y, vectorizer

# def tune_lasso(X, y):
#     """ä½¿ç”¨GridSearchCVå¯»æ‰¾æœ€ä¼˜çš„alphaå‚æ•°"""
#     print("å¼€å§‹è°ƒå‚...")
#     alphas = np.logspace(-5, -1, 10)
#     model = Lasso(max_iter=10000)
#     grid = GridSearchCV(model, {'alpha': alphas}, cv=5, scoring='r2')
#     grid.fit(X, y)
#     print(f"æœ€ä½³ alpha: {grid.best_params_['alpha']:.1e}")
#     print(f"æœ€ä½³äº¤å‰éªŒè¯ RÂ²: {grid.best_score_:.4f}")
#     return grid.best_estimator_

# def report(model, X_test, y_test, vectorizer):
#     """è¯„ä¼°æ¨¡å‹è¡¨ç°å¹¶è¾“å‡ºé‡è¦ç‰¹å¾"""
#     y_pred = model.predict(X_test)
#     mse = mean_squared_error(y_test, y_pred)
#     r2 = r2_score(y_test, y_pred)

#     print(f"\nğŸ“Š æ¨¡å‹æµ‹è¯•é›†å‡æ–¹è¯¯å·®(MSE): {mse:.6f}")
#     print(f"ğŸ“ˆ æ¨¡å‹æµ‹è¯•é›†R2å¾—åˆ†: {r2:.4f}")

#     # è¾“å‡ºé‡è¦ç‰¹å¾
#     coef = model.coef_
#     features = vectorizer.get_feature_names_out()
#     important_features = [(f, c) for f, c in zip(features, coef) if abs(c) > 1e-5]
#     important_features = sorted(important_features, key=lambda x: abs(x[1]), reverse=True)

#     if important_features:
#         print("\nğŸ§  æœ€é‡è¦çš„è¯å’Œå¯¹åº”ç³»æ•°:")
#         for word, weight in important_features[:20]:
#             print(f"{word}: {weight:.6f}")
#     else:
#         print("\nâš ï¸ æ²¡æœ‰éé›¶é‡è¦ç‰¹å¾ï¼ˆæ¨¡å‹å¯èƒ½æ²¡å­¦åˆ°ä»€ä¹ˆï¼‰")

# def plot_sentiment_vs_return(model, X_test, y_test):
#     """æŒ‰é¢„æµ‹å¾—åˆ†åˆ†ç»„ï¼Œç»˜åˆ¶å¹³å‡å›æŠ¥è¶‹åŠ¿å›¾"""
#     predicted_scores = model.predict(X_test)
#     df = pd.DataFrame({'pred_score': predicted_scores, 'actual_return': y_test})
#     # è§£å†³é‡å¤è¾¹ç•Œçš„é—®é¢˜ï¼Œduplicates='drop'
#     df['decile'] = pd.qcut(df['pred_score'], 10, labels=False, duplicates='drop')

#     avg_returns = df.groupby('decile')['actual_return'].mean()

#     plt.figure(figsize=(10, 6))
#     plt.plot(avg_returns.index, avg_returns.values, marker='o', label='å¹³å‡æ¬¡æ—¥å›æŠ¥')
#     plt.title("ğŸ“Š æ¨¡å‹é¢„æµ‹æƒ…æ„Ÿåˆ†æ•° vs å¹³å‡æ¬¡æ—¥å›æŠ¥", fontsize=16)
#     plt.xlabel("é¢„æµ‹æƒ…æ„Ÿå¾—åˆ†åˆ†ç»„ï¼ˆä»è´Ÿé¢åˆ°æ­£é¢ï¼‰", fontsize=14)
#     plt.ylabel("å¹³å‡å®é™…æ¬¡æ—¥å›æŠ¥", fontsize=14)
#     plt.grid(True)
#     plt.xticks(range(len(avg_returns)), [f"Q{i+1}" for i in range(len(avg_returns))], fontsize=12)
#     plt.yticks(fontsize=12)
#     plt.legend(fontsize=12)
#     plt.tight_layout()
#     plt.show()

#     print("\nå„åˆ†ç»„å¹³å‡æ¬¡æ—¥å›æŠ¥ï¼š")
#     for i, r in enumerate(avg_returns):
#         print(f"Q{i+1}: {r:.6f}")

# def main():
#     print("å¼€å§‹åŠ è½½å¹¶é¢„å¤„ç†æ•°æ®...")
#     df = load_and_preprocess("data/reduced_dataset-release.csv")

#     print("è½¬æ¢æ–‡æœ¬æ ¼å¼ï¼Œå‡†å¤‡ç‰¹å¾æå–...")
#     X, y, vectorizer = build_features(df)

#     print("æ‹†åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†...")
#     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

#     print("è®­ç»ƒå¹¶è°ƒå‚ Lasso æ¨¡å‹...")
#     model = tune_lasso(X_train, y_train)

#     print("è¯„ä¼°æ¨¡å‹...")
#     report(model, X_test, y_test, vectorizer)

#     print("åˆ†ææƒ…æ„Ÿå¾—åˆ†ä¸å›æŠ¥çš„å…³ç³»...")
#     plot_sentiment_vs_return(model, X_test, y_test)

# if __name__ == "__main__":
#     main()




# import pandas as pd
# import numpy as np
# from sklearn.feature_extraction.text import TfidfVectorizer
# from sklearn.linear_model import Lasso
# from sklearn.model_selection import train_test_split, GridSearchCV
# from sklearn.metrics import mean_squared_error, r2_score
# from preprocessing import load_and_preprocess

# def join_words(word_list):
#     """å°†è¯åˆ—è¡¨è½¬ä¸ºä¸€å¥å­—ç¬¦ä¸²ä¾›Tfidfå¤„ç†"""
#     return ' '.join(word_list)

# def build_features(df, max_features=5000):
#     """æ„å»ºTF-IDFå‘é‡"""
#     df['text_joined'] = df['cleaned_text'].apply(join_words)
#     df = df[df['text_joined'].str.strip() != '']
#     df = df[df['1_DAY_RETURN'].notna()]
#     print("å‰©ä½™æœ‰æ•ˆæ ·æœ¬æ•°:", len(df))
#     print("\nç¤ºä¾‹æ–‡æœ¬ï¼š\n", df['text_joined'].iloc[0])
    
#     vectorizer = TfidfVectorizer(max_features=max_features)
#     X = vectorizer.fit_transform(df['text_joined'])
#     y = df['1_DAY_RETURN'].values
#     return X, y, vectorizer

# def tune_lasso(X, y):
#     """ä½¿ç”¨GridSearchCVå¯»æ‰¾æœ€ä¼˜çš„alphaå‚æ•°"""
#     print("å¼€å§‹è°ƒå‚...")
#     alphas = np.logspace(-5, -1, 10)
#     model = Lasso(max_iter=10000)
#     grid = GridSearchCV(model, {'alpha': alphas}, cv=5, scoring='r2')
#     grid.fit(X, y)
#     print(f"æœ€ä½³ alpha: {grid.best_params_['alpha']:.1e}")
#     print(f"æœ€ä½³äº¤å‰éªŒè¯ RÂ²: {grid.best_score_:.4f}")
#     return grid.best_estimator_

# def report(model, X_test, y_test, vectorizer):
#     """è¯„ä¼°æ¨¡å‹è¡¨ç°å¹¶è¾“å‡ºé‡è¦ç‰¹å¾"""
#     y_pred = model.predict(X_test)
#     mse = mean_squared_error(y_test, y_pred)
#     r2 = r2_score(y_test, y_pred)

#     print(f"\nğŸ“Š æ¨¡å‹æµ‹è¯•é›†å‡æ–¹è¯¯å·®(MSE): {mse:.6f}")
#     print(f"ğŸ“ˆ æ¨¡å‹æµ‹è¯•é›†R2å¾—åˆ†: {r2:.4f}")

#     # è¾“å‡ºé‡è¦ç‰¹å¾
#     coef = model.coef_
#     features = vectorizer.get_feature_names_out()
#     important_features = [(f, c) for f, c in zip(features, coef) if abs(c) > 1e-5]
#     important_features = sorted(important_features, key=lambda x: abs(x[1]), reverse=True)

#     if important_features:
#         print("\nğŸ§  æœ€é‡è¦çš„è¯å’Œå¯¹åº”ç³»æ•°:")
#         for word, weight in important_features[:20]:
#             print(f"{word}: {weight:.6f}")
#     else:
#         print("\nâš ï¸ æ²¡æœ‰éé›¶é‡è¦ç‰¹å¾ï¼ˆæ¨¡å‹å¯èƒ½æ²¡å­¦åˆ°ä»€ä¹ˆï¼‰")

# def main():
#     print("å¼€å§‹åŠ è½½å¹¶é¢„å¤„ç†æ•°æ®...")
#     df = load_and_preprocess("data/reduced_dataset-release.csv")

#     print("è½¬æ¢æ–‡æœ¬æ ¼å¼ï¼Œå‡†å¤‡ç‰¹å¾æå–...")
#     X, y, vectorizer = build_features(df)

#     print("æ‹†åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†...")
#     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

#     print("è®­ç»ƒå¹¶è°ƒå‚ Lasso æ¨¡å‹...")
#     model = tune_lasso(X_train, y_train)

#     print("è¯„ä¼°æ¨¡å‹...")
#     report(model, X_test, y_test, vectorizer)

# if __name__ == "__main__":
#     main()

