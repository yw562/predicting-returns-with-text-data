from xgboost import XGBClassifier
from sklearn.metrics import classification_report, confusion_matrix, r2_score
from scipy.stats import spearmanr

def train_binary_model(X, y_binary, pos_label):
    neg_weight = 1
    pos_weight = (y_binary == 0).sum() / (y_binary == 1).sum()
    model = XGBClassifier(
        objective='binary:logistic',
        eval_metric='logloss',
        use_label_encoder=False,
        scale_pos_weight=pos_weight
    )
    sample_weights = np.where(y_binary == 1, pos_weight, neg_weight)
    model.fit(X, y_binary, sample_weight=sample_weights)
    return model

def train_dual_binary_models(X, y):
    y_up = (y == 2).astype(int)
    y_down = (y == 0).astype(int)

    model_up = train_binary_model(X, y_up, pos_label=2)
    model_down = train_binary_model(X, y_down, pos_label=0)

    return model_up, model_down

def predict_with_dual_models(model_up, model_down, X):
    pred_up = model_up.predict(X)
    pred_down = model_down.predict(X)

    y_pred = np.full_like(pred_up, fill_value=1)  # default to neutral
    y_pred[pred_up == 1] = 2
    y_pred[pred_down == 1] = 0
    return y_pred

def predict_proba_with_dual_models(model_up, model_down, X):
    proba_up = model_up.predict_proba(X)[:, 1]
    proba_down = model_down.predict_proba(X)[:, 1]
    proba_neutral = 1 - np.maximum(proba_up, proba_down)

    # Normalize to make probabilities sum to 1
    total = proba_up + proba_down + proba_neutral
    return np.vstack([
        proba_down / total,
        proba_neutral / total,
        proba_up / total
    ]).T

def report_dual(model_up, model_down, X_test, y_test, y_test_continuous):
    y_pred = predict_with_dual_models(model_up, model_down, X_test)
    y_proba = predict_proba_with_dual_models(model_up, model_down, X_test)
    scores = y_proba @ np.array([-1, 0, 1])

    print("\nðŸ“Š Classification Report:")
    print(classification_report(y_test, y_pred))
    print("Confusion Matrix:")
    print(confusion_matrix(y_test, y_pred))

    evaluate_ic(y_test_continuous, scores)
    r2 = r2_score(y_test_continuous, scores)
    print(f"ðŸ“‰ RÂ² vs true return: {r2:.4f}")

    correct_positive = ((y_pred == 2) & (y_test == 2)).sum()
    correct_negative = ((y_pred == 0) & (y_test == 0)).sum()
    total_predicted_trades = ((y_pred == 2) | (y_pred == 0)).sum()
    if total_predicted_trades > 0:
        win_rate = (correct_positive + correct_negative) / total_predicted_trades
        print(f"ðŸ† Win Rateï¼ˆèƒœçŽ‡ï¼‰: {win_rate:.2%}")
    else:
        print("âš ï¸ æ²¡æœ‰é¢„æµ‹å‡ºæ­£å‘æˆ–è´Ÿå‘äº¤æ˜“ï¼Œæ— æ³•è®¡ç®—èƒœçŽ‡")

    return scores


# import pandas as pd
# import numpy as np
# import matplotlib.pyplot as plt
# from sklearn.feature_extraction.text import TfidfVectorizer
# from sklearn.model_selection import GridSearchCV
# from sklearn.metrics import classification_report, confusion_matrix, r2_score
# from xgboost import XGBClassifier
# from scipy.stats import spearmanr
# from collections import Counter
# from preprocessing import load_and_preprocess

# def join_words(word_list):
#     return ' '.join(word_list)

# def get_sample_weights(y, weights_dict=None):
#     counter = Counter(y)
#     if weights_dict is None:
#         majority = max(counter.values())
#         return np.array([majority / counter[label] for label in y])
#     else:
#         return np.array([weights_dict.get(label, 1) for label in y])

# def tune_xgboost(X, y):
#     weights_dict = {0: 3, 1: 1, 2: 3}  # ç»™å°‘æ•°ç±»æ›´å¤§æƒé‡
#     sample_weights = get_sample_weights(y, weights_dict)
#     model = XGBClassifier(objective='multi:softprob', num_class=3, eval_metric='mlogloss', use_label_encoder=False)
#     param_grid = {
#         'n_estimators': [100, 200],
#         'max_depth': [3, 5],
#         'learning_rate': [0.01, 0.1],
#         'subsample': [0.8, 1.0]
#     }
#     grid = GridSearchCV(model, param_grid, cv=5, scoring='f1_macro', n_jobs=-1)
#     grid.fit(X, y, sample_weight=sample_weights)
#     return grid.best_estimator_

# def evaluate_ic(y_true_cont, scores):
#     ic = spearmanr(scores, y_true_cont).correlation
#     print(f"ðŸ“ˆ ICï¼ˆInformation Coefficientï¼‰: {ic:.4f}")

# def report(model, X_test, y_test, y_test_continuous):
#     # é¢„æµ‹æ—¶ç»™ä¸‰ä¸ªç±»åˆ«èµ‹å€¼ -1,0,1 å¾—åˆ†ï¼Œç”¨äºŽè¿žç»­ç›¸å…³åˆ†æž
#     scores = model.predict_proba(X_test) @ np.array([-1, 0, 1])
#     y_pred = model.predict(X_test)

#     print("\nðŸ“Š Classification Report:")
#     print(classification_report(y_test, y_pred))
#     print("Confusion Matrix:")
#     print(confusion_matrix(y_test, y_pred))

#     evaluate_ic(y_test_continuous, scores)
#     r2 = r2_score(y_test_continuous, scores)
#     print(f"ðŸ“‰ RÂ² vs true return: {r2:.4f}")

#     # è®¡ç®— Win Rateï¼ˆèƒœçŽ‡ï¼‰
#     correct_positive = ((y_pred == 2) & (y_test == 2)).sum()
#     correct_negative = ((y_pred == 0) & (y_test == 0)).sum()
#     total_predicted_trades = ((y_pred == 2) | (y_pred == 0)).sum()
#     if total_predicted_trades > 0:
#         win_rate = (correct_positive + correct_negative) / total_predicted_trades
#         print(f"ðŸ† Win Rateï¼ˆèƒœçŽ‡ï¼‰: {win_rate:.2%}")
#     else:
#         print("âš ï¸ æ²¡æœ‰é¢„æµ‹å‡ºæ­£å‘æˆ–è´Ÿå‘äº¤æ˜“ï¼Œæ— æ³•è®¡ç®—èƒœçŽ‡")

#     return scores

# def plot_score_vs_return(scores, y_true_cont):
#     df = pd.DataFrame({'score': scores, 'ret': y_true_cont})
#     df['quantile'] = pd.qcut(df['score'], 10, labels=False, duplicates='drop')
#     avg_ret = df.groupby('quantile')['ret'].mean()

#     plt.figure(figsize=(10, 6))
#     plt.plot(avg_ret.index, avg_ret.values, marker='o')
#     plt.title("ðŸ“Š Score Quantile vs Avg. Return")
#     plt.xlabel("Quantile (Q0 = lowest score)")
#     plt.ylabel("Avg. 1-Day Return")
#     plt.grid(True)
#     plt.tight_layout()
#     plt.show()

#     print("\nå„åˆ†ç»„å¹³å‡æ”¶ç›Šï¼š")
#     for i, r in enumerate(avg_ret):
#         print(f"Q{i}: {r:.6f}")

# def build_features(df, vectorizer=None, max_features=5000):
#     df['text_joined'] = df['cleaned_text']  # ä½ æ•°æ®æœ¬èº«å·²ç»æ˜¯é¢„å¤„ç†å¥½çš„æ–‡æœ¬åˆ—è¡¨æˆ–å­—ç¬¦ä¸²
#     df = df[df['text_joined'].str.strip() != '']
#     df = df[df['1_DAY_RETURN'].notna()]
    
#     # åˆ†ç±»æ ‡ç­¾ï¼š2ä»£è¡¨ä¸Šæ¶¨ï¼ˆ>1%ï¼‰ï¼Œ0ä»£è¡¨ä¸‹è·Œï¼ˆ<-1%ï¼‰ï¼Œ1ä»£è¡¨ä¸­æ€§
#     df['label'] = df['1_DAY_RETURN'].apply(lambda x: 2 if x > 0.01 else (0 if x < -0.01 else 1))

#     if vectorizer is None:
#         vectorizer = TfidfVectorizer(max_features=max_features)
#         X = vectorizer.fit_transform(df['text_joined'])
#     else:
#         X = vectorizer.transform(df['text_joined'])
    
#     y = df['label'].values
#     y_continuous = df['1_DAY_RETURN'].values

#     return X, y, y_continuous, vectorizer, df

# def main():
#     print("ðŸ“¦ è¯»å–å¹¶é¢„å¤„ç†æ•°æ®...")
#     df = load_and_preprocess(r"c:\\Users\\yw562\\Downloads\\predicting-returns-with-text-data-master\\predicting-returns-with-text-data-master\\data\\cleaned_aligned_data.csv")
#     stock_list = ["Nike", "eBay", "Reuters", "Netflix", "Amazon"]
#     print("ðŸ“ˆ è‚¡ç¥¨åˆ—è¡¨ï¼š", stock_list)

#     for stock in stock_list:
#         print(f"\nðŸ” æ­£åœ¨å¤„ç†è‚¡ç¥¨ï¼š{stock}")
#         df_stock = df[df['STOCK_CODE'] == stock].copy()
#         if len(df_stock) < 100:
#             print(f"ðŸš« æ ·æœ¬æ•°ä¸è¶³ï¼Œè·³è¿‡ {stock}")
#             continue

#         df_stock = df_stock.sort_values('DATE')
#         split_date = df_stock['DATE'].quantile(0.8)
#         train_df = df_stock[df_stock['DATE'] <= split_date]
#         test_df = df_stock[df_stock['DATE'] > split_date]

#         if len(train_df) < 50 or len(test_df) < 20:
#             print(f"ðŸš« è®­ç»ƒæˆ–æµ‹è¯•æ•°æ®å¤ªå°‘ï¼Œè·³è¿‡ {stock}")
#             continue

#         X_train, y_train, _, vectorizer, _ = build_features(train_df)
#         X_test, y_test, y_test_cont, _, _ = build_features(test_df, vectorizer=vectorizer)

#         print(f"âœ… è®­ç»ƒæ ·æœ¬æ•°: {X_train.shape[0]}ï¼Œæµ‹è¯•æ ·æœ¬æ•°: {X_test.shape[0]}")
#         model = tune_xgboost(X_train, y_train)
#         scores = report(model, X_test, y_test, y_test_cont)
#         plot_score_vs_return(scores, y_test_cont)

# if __name__ == "__main__":
#     main()
